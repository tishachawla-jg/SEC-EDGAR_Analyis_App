{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7q40bdVJ2qQv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import json\n",
        "from bs4 import BeautifulSoup  # We'll use BeautifulSoup to handle HTML\n",
        "\n",
        "def extract_html_content(text_content):\n",
        "    \"\"\" Extracts and cleans HTML content within <TEXT> tags using BeautifulSoup. \"\"\"\n",
        "    try:\n",
        "        soup = BeautifulSoup(text_content, 'html.parser')\n",
        "        # Extract text from HTML, you could adjust what you extract (e.g., get specific elements)\n",
        "        return soup.get_text(separator=' ', strip=True)\n",
        "    except Exception as e:\n",
        "        return f\"Failed to parse HTML: {str(e)}\"\n",
        "\n",
        "def preprocess_and_extract_text(content):\n",
        "    \"\"\" Attempts to extract the content within <TEXT>...</TEXT> tags. \"\"\"\n",
        "    start_tag = '<TEXT>'\n",
        "    end_tag = '</TEXT>'\n",
        "    try:\n",
        "        start_index = content.index(start_tag) + len(start_tag)\n",
        "        end_index = content.index(end_tag)\n",
        "        text_content = content[start_index:end_index]\n",
        "        return extract_html_content(text_content)\n",
        "    except ValueError:\n",
        "        return \"Could not find <TEXT> tags\"\n",
        "\n",
        "def merge_full_submission_texts_to_json(zip_path, output_json_file):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        all_folders = [item for item in z.namelist() if item.endswith('/')]\n",
        "        folders = [f for f in all_folders if f.startswith('AAPL/10-K/') and f.count('/') == 3]\n",
        "        merged_texts = {}\n",
        "\n",
        "        for folder in folders:\n",
        "            file_path = os.path.join(folder, 'full-submission.txt')\n",
        "            if file_path in z.namelist():\n",
        "                with z.open(file_path) as file:\n",
        "                    content = file.read().decode('utf-8')\n",
        "                    normalized_content = preprocess_and_extract_text(content)\n",
        "                    folder_key = folder.strip('/')\n",
        "                    merged_texts[folder_key] = normalized_content\n",
        "\n",
        "        with open(output_json_file, 'w') as f_out:\n",
        "            json.dump(merged_texts, f_out, indent=4)\n",
        "\n",
        "# Usage\n",
        "zip_path = '/content/filings.zip'\n",
        "output_json_file = 'extracted_html_content.json'\n",
        "merge_full_submission_texts_to_json(zip_path, output_json_file)\n",
        "# Write the JSON data to a .txt file in pretty-printed form\n",
        "with open('extracted_html_content.txt', 'w') as txt_file:\n",
        "    json.dump(data, txt_file, indent=4)\n"
      ]
    }
  ]
}